{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "### Actions that can be perfomed using this notebook:\n",
    "- PHOTO\n",
    "    - take a selfie\n",
    "    - take a screenshot\n",
    "    - upload a photo\n",
    "- VIDEO\n",
    "    - upload a video\n",
    "    - do a live video\n",
    "\n",
    "and then apply a filter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **A. Photos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources:\n",
    "- https://www.digitalocean.com/community/tutorials/how-to-detect-and-extract-faces-from-an-image-with-opencv-and-python  \n",
    "- https://stackoverflow.com/questions/48512532/cropping-faces-from-an-image-using-opencv-in-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4pMMKDV7-95J"
   },
   "source": [
    "# 1. Selfie using the computer camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making necessary imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "from utils import *\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "model_path = \"best_model.hdf5\"\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the selfie \n",
    "image_selfie = camera_grab(camera_id=0)\n",
    "plt.imshow(image_selfie)\n",
    "print(\"dtype: {}, shape: {}, range: {}\".format(\n",
    "    image_selfie.dtype, image_selfie.shape, (image_selfie.min(), image_selfie.max())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforming the image \n",
    "new_link = 'image_selfie.jpg'\n",
    "cv2.imwrite('images_source/' + new_link, image_selfie)\n",
    "image_selfie_output = apply_filters(image_name=new_link,\n",
    "                                    model=model,\n",
    "                                    path_to_DL='', \n",
    "                                    filter_name='glasses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the filtered photo \n",
    "plt.imshow(image_selfie_output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Making a screenshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making necessary imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "from utils import *\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "model_path = \"best_model.hdf5\"\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a screenshot \n",
    "! screencapture -T 3 images_source/timedshot.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding filters \n",
    "image_screen_output = apply_filters(image_name='timedshot.jpg',\n",
    "                                    model=model,\n",
    "                                    path_to_DL='', \n",
    "                                    filter_name='glasses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the filtered photo \n",
    "plt.imshow(image_screen_output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Use a photo of one or many people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making necessary imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_to_use = 'a_3_funny1.jpeg'\n",
    "show_image = plt.imread('images_source/' + image_to_use)\n",
    "plt.imshow(show_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding filters \n",
    "image_screen_output = apply_filters(image_name=image_to_use,\n",
    "                                    model=model,\n",
    "                                    path_to_DL='', \n",
    "                                    filter_name='glasses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the filtered photo \n",
    "plt.imshow(image_screen_output)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **B. Videos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Live video with a beard filter moved every 5 images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model\n",
    "model_path = \"best_model.hdf5\"\n",
    "print(f\"Model path : {model_path}.\")\n",
    "model = load_model(model_path)\n",
    "print(\"Model load was successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter\n",
    "santa_filter_original = cv2.imread('filters/santa_filter.png', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_santa_filter(image, model, rotate_beard_bool=True):\n",
    "    #santa_filter_original = cv2.imread('filters/santa_filter.png', -1)\n",
    "    rotate_beard_bool = True\n",
    "    img_copy = np.copy(image)\n",
    "    cropped_images, dimensions = crop_image(image)\n",
    "    counter=0\n",
    "    \n",
    "    # Building global variables as we need to reapply the same beard on multiple successive images \n",
    "    # for the sake of simplicity, we decided that only one person would appear on the video \n",
    "    global x\n",
    "    global y \n",
    "    global right_lip_coords\n",
    "    global top_lip_coords\n",
    "    global shift_beard_left\n",
    "    global shift_beard_top\n",
    "    global santa_filter\n",
    "    \n",
    "    for cropped_image in cropped_images:\n",
    "        x,y,w,h = dimensions[counter]\n",
    "        image_test = resize_image(cropped_image) # resize image for the model\n",
    "        rescaling_factor = cropped_image.shape[0]/96 \n",
    "        image_model = np.reshape(image_test, (1,96,96,1))/255 # normalise pixel value\n",
    "        keypoints = model.predict(image_model)[0]\n",
    "\n",
    "        x_coords = keypoints[0::2]*rescaling_factor\n",
    "        y_coords = keypoints[1::2]*rescaling_factor\n",
    "        left_lip_coords = (int(x_coords[11]), int(y_coords[11]))\n",
    "        \n",
    "        right_lip_coords = (int(x_coords[12]), int(y_coords[12]))\n",
    "        top_lip_coords = (int(x_coords[13]), int(y_coords[13]))\n",
    "        bottom_lip_coords = (int(x_coords[14]), int(y_coords[14]))\n",
    "        left_eye_coords = (int(x_coords[3]), int(y_coords[3]))\n",
    "        right_eye_coords = (int(x_coords[5]), int(y_coords[5]))\n",
    "        brow_coords = (int(x_coords[6]), int(y_coords[6]))\n",
    "\n",
    "        # Scale filter according to keypoint coordinates\n",
    "        beard_width = left_lip_coords[0] - right_lip_coords[0]\n",
    "        beard_length =  int(cropped_image.shape[1]/3*2) # a quarter of the full image\n",
    "        shift_beard_left =  int(0.25*beard_width)\n",
    "        shift_beard_top = int(0.1*beard_length)\n",
    "        scale_beard_factor = 3/2\n",
    "\n",
    "        img_copy = cv2.cvtColor(img_copy, cv2.COLOR_BGR2BGRA)       # Used for transparency overlay of filter using the alpha channel\n",
    "\n",
    "        # Beard filter\n",
    "        santa_filter = cv2.resize(santa_filter_original, (int(beard_width*scale_beard_factor),beard_length))\n",
    "        sw,sh,sc = santa_filter.shape\n",
    "\n",
    "        if rotate_beard_bool == True:\n",
    "            santa_filter = rotate_beard(santa_filter, left_lip_coords, right_lip_coords)\n",
    "\n",
    "        for i in range(0,sw):   # Overlay the filter based on the alpha channel\n",
    "            for j in range(0,sh):\n",
    "                if santa_filter[i,j][3] != 0:\n",
    "                    try:\n",
    "                        img_copy[top_lip_coords[1]+y+i-shift_beard_top, right_lip_coords[0]+x+j-shift_beard_left] = santa_filter[i,j][3]\n",
    "                    except:\n",
    "                        pass\n",
    "        counter+=1\n",
    "    return img_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a video that detects faces and apply the filter every 5 images\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "count = 0\n",
    "ret, image_selfie = video_capture.read()\n",
    "santa_filter = cv2.imread('filters/santa_filter.png', -1)\n",
    "\n",
    "while True:\n",
    "    if (count%5!=0):\n",
    "        ret, image_selfie = video_capture.read()\n",
    "        sw,sh,sc = santa_filter.shape\n",
    "        for i in range(0,sw):   # Overlay the filter based on the alpha channel\n",
    "            for j in range(0,sh):\n",
    "                if santa_filter[i,j][3] != 0:\n",
    "                    try:\n",
    "                        image_selfie[top_lip_coords[1]+y+i-shift_beard_top, right_lip_coords[0]+x+j-shift_beard_left] = santa_filter[i,j][3]\n",
    "                    except:\n",
    "                        pass\n",
    "        cv2.imshow('Video', image_selfie)\n",
    "        \n",
    "    else: \n",
    "        # Capture frame-by-frame\n",
    "        ret, image_selfie = video_capture.read()\n",
    "        img_copy = apply_santa_filter(image=image_selfie, model=model, rotate_beard_bool=True)\n",
    "\n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Video', img_copy)\n",
    "    count +=1\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# When everything is done, release the capture\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Uploading a video and applying filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources: \n",
    "- https://stackoverflow.com/questions/33311153/python-extracting-and-saving-video-frames\n",
    "- https://theailearner.com/2018/10/15/creating-video-from-images-using-opencv-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ffmpeg-python\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from utils import *\n",
    "import ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter\n",
    "santa_filter_original = cv2.imread('filters/santa_filter.png', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_santa_filter(image, model, rotate_beard_bool=True):\n",
    "    #santa_filter_original = cv2.imread('filters/santa_filter.png', -1)\n",
    "    rotate_beard_bool = True\n",
    "    img_copy = np.copy(image)\n",
    "    cropped_images, dimensions = crop_image(image)\n",
    "    counter=0\n",
    "    \n",
    "    # Building global variables as we need to reapply the same beard on multiple successive images \n",
    "    # for the sake of simplicity, we decided that only one person would appear on the video \n",
    "    global x\n",
    "    global y \n",
    "    global right_lip_coords\n",
    "    global top_lip_coords\n",
    "    global shift_beard_left\n",
    "    global shift_beard_top\n",
    "    global santa_filter\n",
    "    \n",
    "    for cropped_image in cropped_images:\n",
    "        x,y,w,h = dimensions[counter]\n",
    "        image_test = resize_image(cropped_image) # resize image for the model\n",
    "        rescaling_factor = cropped_image.shape[0]/96 \n",
    "        image_model = np.reshape(image_test, (1,96,96,1))/255 # normalise pixel value\n",
    "        keypoints = model.predict(image_model)[0]\n",
    "\n",
    "        x_coords = keypoints[0::2]*rescaling_factor\n",
    "        y_coords = keypoints[1::2]*rescaling_factor\n",
    "        left_lip_coords = (int(x_coords[11]), int(y_coords[11]))\n",
    "        \n",
    "        right_lip_coords = (int(x_coords[12]), int(y_coords[12]))\n",
    "        top_lip_coords = (int(x_coords[13]), int(y_coords[13]))\n",
    "        bottom_lip_coords = (int(x_coords[14]), int(y_coords[14]))\n",
    "        left_eye_coords = (int(x_coords[3]), int(y_coords[3]))\n",
    "        right_eye_coords = (int(x_coords[5]), int(y_coords[5]))\n",
    "        brow_coords = (int(x_coords[6]), int(y_coords[6]))\n",
    "\n",
    "        # Scale filter according to keypoint coordinates\n",
    "        beard_width = left_lip_coords[0] - right_lip_coords[0]\n",
    "        beard_length =  int(cropped_image.shape[1]/3*2) # a quarter of the full image\n",
    "        shift_beard_left =  int(0.25*beard_width)\n",
    "        shift_beard_top = int(0.1*beard_length)\n",
    "        scale_beard_factor = 3/2\n",
    "\n",
    "        img_copy = cv2.cvtColor(img_copy, cv2.COLOR_BGR2BGRA)       # Used for transparency overlay of filter using the alpha channel\n",
    "\n",
    "        # Beard filter\n",
    "        santa_filter = cv2.resize(santa_filter_original, (int(beard_width*scale_beard_factor),beard_length))\n",
    "        sw,sh,sc = santa_filter.shape\n",
    "\n",
    "        if rotate_beard_bool == True:\n",
    "            santa_filter = rotate_beard(santa_filter, left_lip_coords, right_lip_coords)\n",
    "\n",
    "        for i in range(0,sw):   # Overlay the filter based on the alpha channel\n",
    "            for j in range(0,sh):\n",
    "                if santa_filter[i,j][3] != 0:\n",
    "                    try:\n",
    "                        img_copy[top_lip_coords[1]+y+i-shift_beard_top, right_lip_coords[0]+x+j-shift_beard_left] = santa_filter[i,j][3]\n",
    "                    except:\n",
    "                        pass\n",
    "        counter+=1\n",
    "    return img_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter name of the video to use and the name that you want to give to the filtered output\n",
    "video_input_name = 'pantin_vic.mov'\n",
    "video_output_name = 'output_pantin_2.mov'\n",
    "video_gif_name = 'pantin_vic.gif'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It was a success\n"
     ]
    }
   ],
   "source": [
    "# Saving all images from the video in the right order \n",
    "vidcap = cv2.VideoCapture('videos_source/' + video_input_name)\n",
    "success,image = vidcap.read()\n",
    "count = 0\n",
    "list_images = []\n",
    "while success:\n",
    "    if count < 10:\n",
    "        count_name = '00' + str(count)\n",
    "    elif count < 100:\n",
    "        count_name = '0' + str(count)\n",
    "    else:\n",
    "        count_name = str(count)\n",
    "    cv2.imwrite(\"videos_source/pantin_vic_images/frame\" + count_name + \".jpg\", image)     # save frame as JPEG file      \n",
    "    success,image = vidcap.read()\n",
    "    list_images.append(\"videos_source/pantin_vic_images/frame\" + count_name + \".jpg\") # the ordered list of all images \n",
    "    count += 1\n",
    "print('It was a success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the model \n",
    "model_path = \"best_model.hdf5\"\n",
    "model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to apply a filter on each image \n",
    "def get_transform(image_name, apply_both=False):\n",
    "    image_selfie = plt.imread(image_name)\n",
    "    if apply_both == False:\n",
    "        return apply_santa_filter(image_selfie, model)\n",
    "    else:\n",
    "        return apply_both_filters(image_selfie, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying a filter on all images \n",
    "for img in list_images:\n",
    "    image = get_transform(img)\n",
    "    cv2.imwrite(img, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Making the new video from all images with a filter \n",
    "(\n",
    "    ffmpeg\n",
    "    .input('videos_source/pantin_vic_images/*.jpg', pattern_type='glob', framerate=20)\n",
    "    .output(video_output_name)\n",
    "    .run()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **C. Extra - Creating GIFs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ez_setup\n",
    "#!pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:  25%|██▌       | 3/12 [00:00<00:00, 17.18it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Building file results/pantin_vic.gif with imageio.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import *\n",
    "clip = (VideoFileClip(video_output_name)\n",
    "        .subclip((0,2.65),(0,3.2))\n",
    "        .resize(0.3))\n",
    "clip.write_gif('results/' + video_gif_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "notebook_vic_v1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
